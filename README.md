# ğŸ§ª IaC Data Platform (Local + Free)

This project is a **mock data platform** built entirely with **Terraform and Docker**, designed to emulate a modern cloud data stack **locally and for free**.

It showcases core data engineering skills: infrastructure as code, orchestration, storage, event-driven compute, and transformations.

This repository contains the example code and documentation referenced in the article [Building a Local Data Platform with Terraform and Docker](https://p-munhoz.github.io/blog/building-local-data-platform-terraform-docker)

---

## ğŸ“¦ Tech Stack

| Component      | Tool/Service        | Emulated with         |
| -------------- | ------------------- | --------------------- |
| Object Storage | AWS S3              | Minio                 |
| Compute (ETL)  | AWS Lambda          | Docker + BashOperator |
| Orchestration  | MWAA / Airflow      | Dockerized Airflow    |
| Message Queue  | SQS/SNS             | LocalStack            |
| Data Warehouse | Redshift / BigQuery | DuckDB                |
| IaC Tool       | Terraform           | Local                 |

---

## ğŸ—º Architecture Overview

```
User / DAG Trigger
   |
   v
Airflow DAG
   |
   |--> Upload CSV to Minio
   |--> Load raw CSV to DuckDB
   |--> [Triggered via SQS] Run Docker ETL
   |--> Load transformed data to DuckDB
```

* **Airflow** orchestrates tasks
* **Minio** simulates S3 object storage
* **ETL step** is a Docker container simulating Lambda
* **DuckDB** acts as the local data warehouse
* **LocalStack** provides SQS queue to trigger the ETL

---

## ğŸš€ How to Run the Project

### 1. Clone the Repo

```bash
git clone https://github.com/YOUR_USERNAME/iac-data-platform.git
cd iac-data-platform
```

### 2. Apply Terraform Infrastructure

```bash
cd terraform
terraform init
terraform apply
```

### 3. Access Airflow

Visit [http://localhost:8080](http://localhost:8080)
Login: `admin` / `admin`

### 4. Create the SQS Queue (once)

```bash
AWS_ACCESS_KEY_ID=test \
AWS_SECRET_ACCESS_KEY=test \
aws --endpoint-url=http://localhost:4566 \
    sqs create-queue \
    --queue-name my-etl-queue \
    --region us-east-1
```

### 5. Upload a Message to SQS

```bash
AWS_ACCESS_KEY_ID=test \
AWS_SECRET_ACCESS_KEY=test \
aws --endpoint-url=http://localhost:4566 \
     sqs send-message \
     --queue-url http://localhost:4566/000000000000/my-etl-queue \
     --message-body "trigger"
```

---

## ğŸ“ Project Structure

```
iac-data-platform/
â”œâ”€â”€ terraform/              # Infra provisioning
â”‚   â”œâ”€â”€ modules/            # Minio, Airflow, LocalStack
â”œâ”€â”€ dags/                   # Airflow DAGs
â”œâ”€â”€ scripts/                # ETL transformation logic
â”œâ”€â”€ data/                   # CSV files and DuckDB storage
â”œâ”€â”€ airflow.Dockerfile      # Custom Airflow image
â”œâ”€â”€ scripts/Dockerfile.etl  # ETL image
â””â”€â”€ README.md
```

---

## âœ¨ What This Project Demonstrates

* ğŸ“¦ Infrastructure-as-Code with Terraform modules
* ğŸ§© Airflow DAGs connected to Dockerized transformations
* ğŸ” Event-driven execution via SQS (LocalStack)
* ğŸ›  Realistic dev environment with local S3 + DuckDB
* âš™ï¸ Simulated Lambda compute via `docker run`

---

## ğŸ§  Ideas to Extend It

* Use GitHub Actions to test provisioning on every commit
* Add unit tests for transformation logic
* Add Prometheus + Grafana for basic monitoring
* Support streaming (Kafka or Kinesis with LocalStack)

---

## ğŸ“œ License

MIT License â€” free to use, fork, adapt, and learn!
